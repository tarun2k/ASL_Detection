# ASL_Detection

A machine learning based application that will read various alphabets and numbers from sign language used by people with disabilities using real-time images from the camera.
Using a text to speech API, the application directly convert the words detected from sign language to audio.

The ASL dataset consists of following alphabets and symbols:
![alt text](https://github.com/tarun2k/ASL_Detection/blob/master/images/vector-language-deaf-mutes-hand-american-sign-asl-alphabet-art-123911624.jpg?raw=true)

The following is the screen that first appears on running the program:
![alt text](https://github.com/tarun2k/ASL_Detection/blob/master/images/GUI.JPG?raw=true)

<b> Open Camera to detect button </b> launches the live camera to detect sign language.

After detecting sentences and words, they appear in the space in front of <b> Words Detected </b> field.

The speaker button speaks out the words detected. 

<b> Clear button </b> clears out the words detected so that application can be used to detect new words and sentences. </b>

<b> Quit button </b> closes the application.

The following is the result of the machine learning model detecting the Letter 'L' on real-time images:
![alt text](https://github.com/tarun2k/ASL_Detection/blob/master/images/Detection_L.JPG?raw=true)

And this is how the word is detected and appears on the GUI:
![alt text](https://github.com/tarun2k/ASL_Detection/blob/master/images/word_detected.JPG?raw=true)
